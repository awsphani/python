https://www.youtube.com/watch?v=PvbT-l0mweY&list=PLf0swTFhTI8otgadSFlL44X_ISa0jvhJQ&index=7
https://www.youtube.com/watch?v=EGQjqcOIjIM&index=8&list=PLf0swTFhTI8o2V96xr0ayFWKfFJ2PvxHM
https://www.youtube.com/watch?v=8N1VZ6muG8U&index=12&list=PLf0swTFhTI8q0x0V1E6We5zBQ9UazHFY0
https://www.youtube.com/watch?v=aLt6n6shqJw&list=PLf0swTFhTI8pronNK7Gm-isKX7tdNb0Go

https://www.youtube.com/watch?v=NQE7q8PoVSE

https://www.youtube.com/watch?v=hw00BMez2h0 . >pycharm with spark

https://www.youtube.com/watch?v=pddtwAtUsbU&index=16&list=PLf0swTFhTI8otgadSFlL44X_ISa0jvhJQ

https://www.youtube.com/watch?v=H0aByUM_cZI&list=PLf0swTFhTI8pronNK7Gm-isKX7tdNb0Go&index=44
https://github.com/Pushkr/Apache-Spark-Hands-On
https://www.youtube.com/watch?v=IRSpBce_T5Y&list=PLf0swTFhTI8rJvGpOp-LujOcpk-Rlz-yE

REPL: read evaluate print loop
print ("Hello World")    

sh-4.2$ python                                                                  
Python 2.7.12 (default, Sep  1 2016, 22:14:00)                                  
[GCC 4.8.3 20140911 (Red Hat 4.8.3-9)] on linux2                                
Type "help", "copyright", "credits" or "license" for more information.          
>>> print ("Hello World")                                                       
Hello World    

>>l = range (1,20)                                                                                                                                                                                                                                                                         
>>> l                                                                                                                                                                                                                                                                                        
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]

>>> res=0                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                      
>>> for i in l:                                                                                                                                                                                                                                                                              
...  res+=i                                                                                                                                                                                                                                                                                  
...                                                                                                                                                                                                                                                                                          
>>> res                                                                                                                                                                                                                                                                                      
190                                                                                                                                                                                                                                                                                          


def sum(func,lb,ub):
  tot=0
  while(lb<=ub):
    tot+=func(lb)
    lb+=1
   return tot
   
 def id(i):
  return i
  
 def sqr(i)
   return i*i
  
  sum(id,1,10)
  sum(sqr,1,10)
  
  
  sum(lambda i: i*i, 1,10)
  
  lists:
  
  l = [1,2,3,1,2,7,8]
  
  l[0]=1
  l[1]=2
  l[-1] last elem
  l[2:5]
  s= set(l)
  
  removes duplicates and sets cannot do s[0] s[1]...set obj doesnot support indexing
  
  help(l)
  help(s)
  
  >>> l =[1,2,1,2,5,6,7]                                                                              
>>> l                                                                                               
[1, 2, 1, 2, 5, 6, 7]                                                                               
>>> s= set(l)                                                                                       
>>> s                                                                                               
set([1, 2, 5, 6, 7])                                                                                
>>> l1= [8,9,10]                                                                                    
>>>                                                                                                 
>>> l.append(3,4,5)                                                                                 
Traceback (most recent call last):                                                                  
  File "<stdin>", line 1, in <module>                                                               
TypeError: append() takes exactly one argument (3 given)                                            
>>> l.append(3)                                                                                     
>>> l                                                                                               
[1, 2, 1, 2, 5, 6, 7, 3]                                                                            
>>> l.append(l1)                                                                                    
>>> l                                                                                               
[1, 2, 1, 2, 5, 6, 7, 3, [8, 9, 10]]                                                                
>>> l.extend(l1)                                                                                    
>>> l                                                                                               
[1, 2, 1, 2, 5, 6, 7, 3, [8, 9, 10], 8, 9, 10]                                                      
>>>                                                                                                 
    
 l.append(object)  -- append object to end
 l.extend(iterable) --extend list by appending elems from iterable
 >>> l[2:5]                                                                                          
[1, 2, 5] 
l[2:5][0]
1

>>> d= {1:"hello",2:"world"} 
>>> d[1]="folks"                                                                                    
>>> d                                                                                               
{1: 'folks', 2: 'world'}  
>>> d.keys()                                                                                        
[1, 2]                                                                                              
>>> d.values()                                                                                      
['folks', 'world']                                                                                  
>>> d.items()                                                                                       
[(1, 'folks'), (2, 'world')]                                                                        
>>> type(d.items())                                                                                 
<type 'list'>  

 map filter reduce
 
 map(function, sequence[, sequence, ...]) -> list                                                
                                                                                                    
    Return a list of the results of applying the function to the items of                           
    the argument sequence(s).  If more than one sequence is given, the                              
    function is called with an argument list consisting of the corresponding                        
    item of each sequence, substituting None for missing values when not all                        
    sequences have the same length.  If the function is None, return a list of                      
    the items of the sequence (or a list of tuples if more than one sequence).     
                                                                                                                            
filter(...)                                                                                                                 
    filter(function or None, sequence) -> list, tuple, or string                                                            
                                                                                                                            
    Return those items of sequence for which function(item) is true.  If                                                    
    function is None, return the items that are true.  If sequence is a tuple                                               
    or string, return the same type, else return a list.     
  
 reduce(...)                                                                     
    reduce(function, sequence[, initial]) -> value                              
                                                                                
    Apply a function of two arguments cumulatively to the items of a sequence,  
    from left to right, so as to reduce the sequence to a single value.         
    For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates           
    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items    
    of the sequence in the calculation, and serves as a default when the        
    sequence is empty.  
    
>>> l= range(1,10)                                                                                                          
>>> f=filter(lambda i: i%2==0,l)                                                                                            
>>> l                                                                                                                       
[1, 2, 3, 4, 5, 6, 7, 8, 9]                                                                                                 
>>> f                                                                                                                       
[2, 4, 6, 8]                                                                                                                
>>> m=map(lambda i: i*i,f)                                                                                                  
>>> m                                                                                                                       
[4, 16, 36, 64]                                                                                                             
>>> r=reduce(lambda total,element: total+element,f)                                                                         
>>> r                                                                                                                       
20                                                                                                                          
>>> r=reduce(lambda total,element: total+element,f,0)                                                                       
>>> r                                                                                                                       
20                                                                                                                          
>>> r=reduce(lambda total,element: total+element,f,10)                                                                      
>>> r                                                                                                                       
30                                                                                                                          
>>> k=[1,2,3,4]                                                                                                             
>>> r1=reduce(lambda total,element: total*element)                                                                          
Traceback (most recent call last):                                                                                          
  File "<stdin>", line 1, in <module>                                                                                       
TypeError: reduce expected at least 2 arguments, got 1                                                                      
>>> r1=reduce(lambda total,element: total*element,k)                                                                        
>>> r1                                                                                                                      
24                                                                                                                          
>>> r1=reduce(lambda total,element: total*element,k,5)                                                                      
>>> r1                                                                                                                      
120                                              
 
 https://spark.apache.org/downloads.html
 https://www.apache.org/dyn/closer.lua/spark/spark-1.6.3/spark-1.6.3-bin-hadoop2.6.tgz
 terminal
 javac -version 
 1.7 or greater
 
 copy from downloads to home in mac
 
 APQYHTD6485432:~ pkum60$ cp ~/Downloads/spark-2.2.0-bin-hadoop2.7.tgz  .
APQYHTD6485432:~ pkum60$ cp ~/Downloads/spark-1.6.3-bin-hadoop2.6.tgz  .
APQYHTD6485432:~ pkum60$ tar xvf spark-1.6.3-bin-hadoop2.6.tgz 
APQYHTD6485432:~ pkum60$ tar xvf spark-2.2.0-bin-hadoop2.7.tgz 
softlink
APQYHTD6485432:~ pkum60$ ln -s spark-1.6.3-bin-hadoop2.6 spark

APQYHTD6485432:~ pkum60$ nano .bash_profile
export SPARK_HOME=/Users/pkum60/spark
export PATH=$PATH:$SPARK_HOME/bin
cntrl+o enter, ctl+x ->nano

APQYHTD6485432:~ pkum60$ . ~/.bash_profile
 APQYHTD6485432:~ pkum60$ spark-shell
 
https://coolestguidesontheplanet.com/add-shell-path-osx/
etc/hadoop/conf: core-site.xml-> can find namenode  in .  details fs.defaultFS
 hdfs-site.xml-> will have blksize and replication info

 
 
 size, block info,replication,
 hadoop fs -du -s -h  s3a://nke-stg/dv/ck/plans/workouts_metadata/downloaded_date=2017-11-27/content/
 hdfs fsck s3a://nke-stg/dv/ck/plans/workouts_metadata/downloaded_date=2017-11-27/content/ -files -blocks -locations
 
 
 yarn-site.xml ->get 
 
 yarn.resourcemanager.webapp.address
 
 etc/spark/conf spark-env.sh
 
 # Options read in YARN client mode
# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
# - SPARK_EXECUTOR_INSTANCES, Number of executors to start (Default: 2)
# - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).
# - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)
# - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)

 can increase SPARK_EXECUTOR_CORES based on the no of cores we have in cluster and the data being processed
   
  get daily rev of product for closed and completed orders, date asc, rev in desc
  broadcast peoducts and perform lkp in to broadcasted hashmap
  get no of closed and completed orders when data is being filtered
  
  spark-shell --master yarn --conf spark.ui.port=12345
  or
  pyspark --master yarn --conf spark.ui.port=12562
  
  to list all control args
  pyspark --help
  
  pyspark --master yarn --conf spark.ui.port=12562 --num-executors 1 --executors-memory 2G
  to pass control args programmatically 
  
  >>>sc.stop()
  
  from pyspark import SparkConf, SparkContext
  conf1 = SparkConf().setMaster("yarn-client").setAppName("Testing").set("spark.ui.port","12445")
  sc = SparkContext(conf=conf1)
  
  use tracking url and paste in browser to check job details
  
  RDD : data is distributed by dividing the dataset(collection) in to partition based on data and tasks in the memory and is resilient as if something is lost in memory spark will create new task and knows which patition and executes again
  so its recovered so resilient
  
  rdd from collection and file
  
  help(sc)
  
  p=range(1,1000)
  sc.parallelize(p)
  
  read from local file path usin python , this will get list and, splitlines splits bases on newline char
  
  productsRaw = open("/data/retail_db/products/part-00000").read().splitlines()
  productsRaw[0]
  
  
  read file from hdfs
  orders = sc.textFile("user/awsphani/retail_db/orders") ->transformation( map,filter,flatmap,union,
  rders.toDebugString() will give the DAG details when transformations are done
  actions:
  orders.count() ->actions
  orders.collect() will take entire rdd distrubuted at multiple nodes in cluster and create single thread python array at driver,
  normally driver is 1GB and you will face memory issues if the rdd is larger than 1 gb lets say 10 TB
  orders.first() ->returns one element of its datatype here unicode string
  orders.take(5)->returns list of unicode strings
 
  
  after action like count, collect(avoid as possible), take(4) or once job is completed , rdd might get flushed out, to make it persist for using RDD multiple times in same session use as below
  
  from pyspark import StorageLevel
  orders.persists(MEMORY_ONLY or MEMORY_ONLY_SER, DISK_ONLY, MEMORY_AND_DISK,....) 
    or
  orders.cache()->MEMORY_ONLY
  
  get daily rev of product for closed and completed orders, date asc, rev in desc
  broadcast peoducts and perform lkp in to broadcasted hashmap
 
 
########## get no of closed and completed orders when data is being filtered
 
  orders = sc.textFile("user/awsphani/retail_db/orders")
  for order in orders.take(10): print(order)
  //get orders status which is 4th column, indexing start at 0 for lists
  ordersStatuses = orders.map(lambda order: order.split(",")[3])
  ordersStatuses.take(2)
  //get all distict orders status to see what are the orderstatus values
  
  for oderStatus in ordersStatuses.distinct().collect(): print(oderStatus)
  
  //filter for getting only closed and complete
  
  ordersFiltered = orders.filter(lambda order: order.split(",")[3]=="COMPLETE" or order.split(",")[3]=="CLOSED")
  ordersFiltered.count()

########## get no of closed and completed orders using accumulators
  
  ordersCompletedCount=0
  def isComplete(order,ordersCompletedCount):
    isCompleted=order.split(",")[3]=="COMPLETE" or order.split(",")[3]=="CLOSED"
    if(isCompleted):
      ordersCompletedCount+=1
    return ordersCompletedCount  
    
    
  ordersFiltered = orders.filter(lambda order: isComplete(order,ordersCompletedCount))
  ordersFiltered.count() >this will be zero as ordersCompletedCount=0 init is part of driver prgm and lambda is executed at different  executor nodes

so to get this right pass and track , we have to use accumulator

 ordersCompletedCount= sc.accumulator(0)
 ordersCompletedCount.value
 
 def isComplete(order,ordersCompletedCount):
    isCompleted=order.split(",")[3]=="COMPLETE" or order.split(",")[3]=="CLOSED"
    if(isCompleted):
      ordersCompletedCount.add(1)
    return ordersCompletedCount  
  
  ordersFiltered = orders.filter(lambda order: isComplete(order,ordersCompletedCount))
  ordersFiltered.count() >this will return correct value;
  
 ################### for joins we need our data in key value pairs: for orders and orderItems here
  
  key value pairs -> key from orders is order_id and value whatever fields we need, order_date
 joins with (K,V) and (K,W) retruns  tuple (K,(V,W)) pairs
  
  ordersMap= ordersFiltered.map(lambda order: (int(order.split(",")[0]),order.split(",")[1]))
  ordersMap.first()-> (1,u'2013-07-25 00:00::00.0') tuple
  
  we need order_is as key and tuple of product_id and revenue as value
 
  orderItems = sc.textFile("user/awsphani/retail_db/order_Items")
  orderItemsMap=orderItems.map(lambda orderItem: (int(orderItem.split(",")[1]),/
  (int(order.splitItem(",")[2]),float(order.splitItem(",")[4]))))
  
  orderItemsMap.first() ->(1,(100, 200.57) tuple
  
  
  ordersJoin = ordersMap.join(orderItemsMap)
  ordersJoin.first() -> (1,(u'2013-07-25 00:00::00.0',(100, 200.57)))
  
  get records in orders that are not in orderitems
  
  ordersLeftOuterJoin=ordersMap.leftOuterJoin(orderItemsMap) -> this will all recs from orders and will have null or none for records that are not in orderItems 
  
  example
     (1,(u'2013-07-25 00:00::00.0',(100, 200.57)))
     (1,(u'2013-07-25 00:00::00.0',(101, 700.57)))
     (2,(u'2013-07-25 00:00::00.0',(None))) = (2,(u'2013-07-25 00:00::00.0',None))
  
  r=  (2,(u'2013-07-25 00:00::00.0',None))
     r[0]=2
     r[1]=(u'2013-07-25 00:00::00.0',None)
r[1][0]=u'2013-07-25 00:00::00.0'
r[1][1]=None (will not print)

ordersWithNoOrderItems=ordersLeftOuterJoin.filter( lambda OrderItemElem: OrderItemElem[1][1]==None)
  
Aggreagte Data:

ReduceByKey- better performance as it uses combiner
aggregateByKey -betterperf, combiner and reducer logic are different
groupByKey-poor perf, use only if RBK,ABK cant be used.

1+2+3+4+5+6+7+8+9+10-> 1+2+3=6 . 4+5+6=15 . 7+8+9+10=34 (combiner) => . 6+15+34(reducer)

getdata in k,v pairs with date+prodid as key revenue as vals using ordersJoin

ordersJoin.first() -> (1,(u'2013-07-25 00:00::00.0',(100, 200.57)))


ordersJoinMap=ordersJoin.map(lambda order:((order[1][0],order[1][1][0]),order[1][1][1])

dailyrevPerProdId=ordersJoinReduce=ordersJoinMap.reduceByKey(lambda tot,rev:tot+rev)

aggregateByKey

get dailyrevPerProdId along with product count 


dailyrevPerProdIdwithProdCount=ordersJoinMap.\
aggregateByKey((0.0,0),
lambda:inter,revenue:(inter[0]+revenue,inter[1]+1),
lambda final,inter:(final[0]+inter[0],final[1]+inter[1])
)


products= open("/data/retail_db/products/part-000000).splitlines()
products[0]
productsRDD =products.parallelize() ->converts list to rdd
productsMap=productsRDD.map(lambda product:(int(product.split(",")[0])product.split(",")[2]))

productId as key, date,rev as-> (product_ID,(date,rev))
dailyrevPerProdIdMap=dailyrevPerProdId.map(lambda rec:(rec[0][1],(rec[0][0],rec[1])))

now can join with products

dailyrevPerProdJoinProducts=dailyrevPerProdIdMap.join(productsMap)
->(prodId,((u'date',rev),prodname))
dailyrevPerProdName=dailyrevPerProdJoinProducts.map(lambda rec:rec[1])
->((u'date',rev),prodname))

broadcast var: when you want to join big dataset with small data set, create hash of small ds as lookup rather than join

get daily rev per product using BC var

products= open("/data/retail_db/products/part-00000").read().splitlines()

get prodid and prodname from abopve py list

productsMap= map(lambda product: (int(product.split(",")[0]),product.split(",")[2]),products)
productsMapdict=dict(productsMap)

#can get prodname using key prodid
productsMapdict[1345]
#broadcast
productsBV=sc.broadcast(productsMapdict)
productsBV.value(1345)

ordersJoin.first() -> (1,(u'2013-07-25 00:00::00.0',(100, 200.57)))


ordersJoinMap=ordersJoin.map(lambda order:((order[1][0],productsBV(order[1][1][0])),order[1][1][1])

dailyrevPerProdNName=ordersJoinReduce=ordersJoinMap.reduceByKey(lambda tot,rev:tot+rev)

dailyrevPerProdNName.first()->((u'date',"prodname"),80.0)


BCV will be propogated to all tasks and if its needed it will be available to use in any of those tasks

sorting:

for i in dailyrevPerProdNName.sortByKey().take(10): print(i)

we want sorting date by asc and desc by rev, above we have to change key as (date,rev) and negate rev so key will be (date,rev)


dailyrevPerProdNNameMap=dailyrevPerProdNName.map(lambda order: ((order[0][0],-order[1]),order[0][1]))
dailyrevPerProdNNameMapSrt=dailyrevPerProdNNameMap.sortByKey()
for i in dailyrevPerProdNNameMapSrt.take(100):print(i)

dailyrevPerProdNNameMapSrtResults=dailyrevPerProdNNameMapSrt.map(ambda order: (order[0][0]+","+ str(-order[0][1])+","+order[1])

for i in dailyrevPerProdNNameMapSrtResults.take(100):print(i)

save in hdfs:
dailyrevPerProdNNameMapSrtResults.saveAsTextFile("/usr/awsphanii/dailyrevforprod")



hive> set hive.metastore.warehouse.dir =>/apps/hive/warehouse ->default hdfs location for hive


pyspark --master yarn --conf spark.ui.port=12562 --num-executors 1 --executors-memory 2G
>> sqlContext.sql("use dabtabse_name");
df=sqlContext.sql("show tables").show;
for i in sqlContext.sql("describe formatted orders").collect():print(i);

sqlContext.sql("select * from orders limit 10").show;



hive language manual:
hive>

create  table orders
(
order_id int,
order_date string,
order_customer_id int,
order_status string
)
row format delimited fields terminated by ','
stored as textfile;


LOAD DATA LOCAL INPATH '/home/awsphani/data/retail_db/orders' into table orders

dfs -ls /apps/hive/warehouse/awsphani_db/orders

create  table order_items
(
order_item_id int,
order_item_order_id int,
order_item_product_id int,
order_item_quantity int,
order_item_subtotal float,
order_item_product_price float
)
row format delimited fields terminated by ','
stored as textfile;

LOAD DATA LOCAL INPATH '/home/awsphani/data/retail_db/order_items' into table orders

create  table orders_orc
(
order_id int,
order_date string,
order_customer_id int,
order_status string
)stored as orc;


create  table order_items_orc
(
order_item_id int,
order_item_order_id int,
order_item_product_id int,
order_item_quantity int,
order_item_subtotal float,
order_item_product_price float
)
stored as orc;


insert into table orders_orc select * from orders
insert into table order_items_orc select * from order_items

hive> show functions;
substr,instr,like,rlike,length,lcase or lower,ucase or upper,rpad,lpad,initcap,split,cast,index

describe function substr;

hive>select index(split("hello how are you",' '),2);


agg funct:> count, sum, avg,min,max

select count(distinct order_status) from orders;

current_date,current_timestamp, date_format(currentdate, 'y'),day(currentdate),todate(current_timestamp)
to_unix_timestamp(current_timestamp), to_date(from_unix_timestamp(1488484848499));date_add(current_date,10)

case

select order_status,
        case order_status
                         when 'CLOSED'  then 'No Action' 
                         when 'COMPLETE'  then 'No Action'
                         else 'RISKY'
                         end from orders limit 10;


select order_status,
             case 
                         when order_status in ('CLOSED','COMPLETE')  then 'No Action' 
                         when order_status in ('ON_HOLD,','PENDING')  then 'Pending Action' 
                         else 'RISKY'
                         end from orders limit 10;

select nvl(order_status,'status missing') from orders;

row level transformation:data standardization,cleansing,obfuscation

select cast(concat(substr(order_date,1,4),substr(order_date,6,2)) as int) from orders limit 10;

select cast(date_format(order_date,"YYYYMM) as int) from orders limit 10;

select o.*,c.* from orders o, customers c 
where o.customer_id =c.customer_id
limit 10;

select o.*,c.* from orders o 
join customers c 
on o.customer_id =c.customer_id limit 10;

get all customers who dont have orders

select o.*,c.* from orders o 
right outer join customers c 
on o.customer_id =c.customer_id where o.customer_id is null limit 10;

          or
          
select * from customers where customer_id not in(select distinct order_customer_id from orders)

aggregations:

revenueperorder is derived field, derived flds cannot be used part of where clause like revenueperorder>1000

WHERE,GROUP BY, HAVING, ORDER BY --->IS THE ORDER

select o.order_id, o.order_date,o.order_status,sum(oi.order_item_subtotal) as revenueperorder
from orders o join order_items oi 
on o.order_id=oi.order_item_order_id
where o.order_status in('COMPLETE','CLOSED')
group by o.order_id,o.order_date,o.order_status
having sum(oi.order_item_subtotal) >=1000

REVENUE PER DAY;

select o.order_date,round(sum(oi.order_item_subtotal),2) as revenueperDAY
from orders o join order_items oi 
on o.order_id=oi.order_item_order_id
where o.order_status in('COMPLETE','CLOSED')
group by o.order_date

sorting

select o.order_id, o.order_date,o.order_status,sum(oi.order_item_subtotal) as revenueperorder
from orders o join order_items oi 
on o.order_id=oi.order_item_order_id
where o.order_status in('COMPLETE','CLOSED')
group by o.order_id,o.order_date,o.order_status
having sum(oi.order_item_subtotal) >=1000
order by o.order_id,revenueperorder desc;

not global sort, but will sort revenue per order for each date -> better performance than order by: use distribute by sort by clause

data might be sorted randomly by date but revenue pero order will be  sorted correctly per date

select o.order_id, o.order_date,o.order_status,sum(oi.order_item_subtotal) as revenueperorder
from orders o join order_items oi 
on o.order_id=oi.order_item_order_id
where o.order_status in('COMPLETE','CLOSED')
group by o.order_id,o.order_date,o.order_status
having sum(oi.order_item_subtotal) >=1000
distribute by o.order_date sort by o.order_date,revenueperorder desc;


SETS
similar dataset for set operations -> union(no duplicates), unionall(dupes also included)

Agg:
Analytic funcs


get %rev for each orderitem having same orderids ; 
if we need more cols and those cols not part of group by we use partition by clause 
so for every entry of order_item we will get its corresponding orderid,orderdate,orderstatus, 
orderitems subtotal from orderitems and then aggregated revenue per orderid  using partition on orderid


select o.order_id, o.order_date,o.order_status,oi.order_item_subtotal,
round(sum(oi.order_item_subtotal) over(partition by o.order_id),2)  revenueperorder,
oi.order_item_subtotal/round(sum(oi.order_item_subtotal)(partition by o.order_id),2) pctrevnue
from orders o join order_items oi 
on o.order_id=oi.order_item_order_id


we cannot put where condition on derived cols like revenueperorder >=1000, so we need to nest the query
nested query needs alias always
select * from
(select o.order_id, o.order_date,o.order_status,oi.order_item_subtotal,
round(sum(oi.order_item_subtotal) over(partition by o.order_id),2)  revenueperorder,
oi.order_item_subtotal/round(sum(oi.order_item_subtotal)(partition by o.order_id),2) pctrevnue,
round(avg(oi.order_item_subtotal) over(partition by o.order_id),2)  avg_rev,
rank() over(partition by o.order_id order by oi.order_item_subtotal desc ) rnk_rev,
dense_rank() over(partition by o.order_id order by oi.order_item_subtotal desc ) dense_rnk_rev,
percent_rank() over(partition by o.order_id order by oi.order_item_subtotal desc ) pct_rnk_rev,
row_number() over(partition by o.order_id order by oi.order_item_subtotal desc ) rownum_orderby_rev
row_number() over(partition by o.order_id ) rownum_rev
from orders o join order_items oi 
on o.order_id=oi.order_item_order_id
where o.order_status in('COMPLETE','CLOSED'))q
where revenueperorder>=1000
order by order_id,revenueperorder desc,rnk_rev;


windowing function: LEAD,LAG,FIRST_VALUE,LAST_VALUE

lead-fetch nxt rec within group, lag-prev value within grp.

select * from
(select o.order_id, o.order_date,o.order_status,oi.order_item_subtotal,
round(sum(oi.order_item_subtotal) over(partition by o.order_id),2)  revenueperorder,
oi.order_item_subtotal/round(sum(oi.order_item_subtotal)(partition by o.order_id),2) pctrevnue,
round(avg(oi.order_item_subtotal) over(partition by o.order_id),2)  avg_rev,
rank() over(partition by o.order_id order by oi.order_item_subtotal desc ) rnk_rev,
dense_rank() over(partition by o.order_id order by oi.order_item_subtotal desc ) dense_rnk_rev,
percent_rank() over(partition by o.order_id order by oi.order_item_subtotal desc ) pct_rnk_rev,
row_number() over(partition by o.order_id order by oi.order_item_subtotal desc ) rownum_orderby_rev
row_number() over(partition by o.order_id ) rownum_rev,
lead(oi.order_item_subtotal) over(partition by o.order_id order by oi.order_item_subtotal desc)  lead_order_item_subtotal,
lag(oi.order_item_subtotal) over(partition by o.order_id order by oi.order_item_subtotal desc)  lag_order_item_subtotal,
first_value(oi.order_item_subtotal) over(partition by o.order_id order by oi.order_item_subtotal desc)  first_order_item_subtotal,
last_value(oi.order_item_subtotal) over(partition by o.order_id order by oi.order_item_subtotal desc)  last_order_item_subtotal
from orders o join order_items oi 
on o.order_id=oi.order_item_order_id
where o.order_status in('COMPLETE','CLOSED'))q
where revenueperorder>=1000
order by order_id,revenueperorder desc,rnk_rev;

sqlContext.sql("select * from ordersschema.orders").show(10,False) -> connects to hive table and ran sql


sqlContext.sql("select * from ordersschema.orders").printSchema()

from pyspark.sql import row
ordersRDD=sc.textfile("/public/retail_db/orders")
for i in ordersRDD.take(10): print(i)
type(ordersRDD) =>RDD

#to convert each element of rdds to structured row, use Row; rdd dont have structure, df will have structure
ordersDF=ordersRDD.map(lambda o: Row(order_id=int(o.split(",")[0]),order_dt=o.split(",")[1],order_custid=int(o.split(",")[2]),orderststus=o.split(",")[3])).toDF()

ordersDF.show()

created df out of RDD, need to create temptable and can query use registerTempTable

ordersDF.registerTempTable("ordersDF_table");

sqlContext.sql("select * from ordersDF_table").show(10,False)


productsRaw=open("/orders/retail_db/products/part-00000").read().splitlines()
type(productsRaw)-=>list

productsRDD=sc.parallelize(productsRaw)
for i in productsRDD.take(10): print(i)

productsDF=productsRDD.\
map(lambda o: Rowproduct_id=int(o.split(",")[0]),product_name=o.split(",")[2]))./
toDF()

productsDF.registerTempTable("productsDF_table");

sqlContext.sql("select * from productsDF_table").show(10,False)




















jdbc-pycharm:
itversity.com/courses/learn-python-for-spark/
itversity.com/topic/implementing-the-application-using-python/
from cygwin
$pip2 install mysql-connector-python-rf
pycharm->python iterapretor->file:///System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python
c/python27/scripts
$ ./pip install . mysql-coonector-python-rf

https://www.youtube.com/watch?v=1ji8lqiBJe0
pycharm->python iterapretor-> +, mysql-coonector-python-rf->install pkg 



$scp -r /Users/pkum60/PycharmProjects/pythondemo awsphani@gw01.itversity.com:~

spark-submit \
--master yarn \
--conf spark.ui.port=12589 \
sparkdemo/src/main/python/pythondemo/DailyRevenuePerCustomer.py \
yarn-client /public/retail_db/ /data/retail_db/customers/part-00000 /user/awsphani/dailyrevenuepercust

10gb ,blksize 128mb,100 million recs=>80 partitions;mapPartitions will be invoked 80 times;whereas map will be 100 million times


default behaviour: no of tasks for subsequent task depends on prev tsk stages;
1stg took 9 tasks,2nd stage will also takes 9 tasks(default behaviour), we can control 2nd stg tasks by using numtasks...
with agg funcs we can use numtasks to reduce or increase numtasks,=>at func level
coalesce(numPatitions)->only to decrease num of partitions(global level),useful after filtering large datasets

mapPatitions(func) : func in iretable<i>=>iterable<U>;this func will be invoked once per every partition.


map-100millions times invoked
ordersMap=orders.map(lambda order:(lambda rec:(rec.split(",")[3],1))

#python lambda inside MP below,ivoked 80 times
ordersMapp=orders.mapPartitions(lambda part:map(lambda rec:(rec.split(",")[3],1),list(part)))

ex:regex class/obj and can apply this on patition level

repartition(numPartitiions):-to create more or less partitions by reshuffling the data in rdd over n/w
without applying any transformations.

orders.cache()-> MEMORY_ONLY,

orders.persists()-> MEMORY_ONLY,DISK_ONLY,MEMORY_AND_DISK,MEMORY_ONLY_SER....
old unused rdds will be removed automaticallyin timely manner, else we can use

RDD.unpersists()




normally for sc,we use 2 executors and num of tasks to process data is eq to no of blks in the file sys
in all the stages.

but when sql is involved shuffling no of tasks is 200 by default, 200 might be overkill for smaller datasets;

to control no of tasks in sparksql,when we have hive table on data,or we have to define struct on top of data using pandas
sqlContext.setConf("spark.sql.shuffle.partitions","2")
  
orders=sc.textFile("/public/retail_db/orders")->rdd
rdd to df

from pyspark.sql import SQLContext,Row
ordersmap=orders.map(lambda o: Row(order_id=int(o.split(",")[0]),order_date=o.split(",")[1],order_cust_id=int(o.split(",")[2],
order_status=o.split(",")[3])).toDF()

ordersmap.show(20)
ordersmap.printSchema()
ordersmap.registerTempTable("ordersTbl");
sqlContext.sql("select * from ordersTbl").show(10)





















  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
   



